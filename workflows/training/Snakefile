import os

configfile: "config.yaml"

TRAIN_FILE = config.get("input", {}).get("data", "").get("train", "")
TEST_FILE = config.get("input", {}).get("data", "").get("test", "")
HP_DIR = config['refit']['hyperparams']
PREFIX = config["output"]["prefix"]
OUT_DIR = os.path.join(config["output"]["dir"], f'job_{PREFIX}')
PLATT_MODEL = (
    config['predict']['platt_model'] 
    if config['predict']['platt_model'] not in (None, "None") 
    else f'{OUT_DIR}/refit/models/{PREFIX}_train_refit_shaprefit_plattscalemodel.json'
)

rule all:
    input:
        f'{OUT_DIR}/refit/importances/{PREFIX}_train_refit_imp.csv', # train data: importance scores
        f'{OUT_DIR}/refit/importances/{PREFIX}_train_refit_shap_colnames.csv',
        directory(f'{OUT_DIR}/refit/importances/{PREFIX}_train_refit_shap_main.zarr'),
        f'{OUT_DIR}/refit/models/{PREFIX}_train_refit_origrefit_xgbmodel.json', # train data: models
        f'{OUT_DIR}/refit/models/{PREFIX}_train_refit_shaprefit_xgbmodel.json',
        f'{OUT_DIR}/refit/predictors/{PREFIX}_train_refit_used_cols.csv', # train data: predictors
        f'{OUT_DIR}/predict/predictions/{PREFIX}_test_predict_y_pred.csv', # test data: predictions
        f'{OUT_DIR}/predict/importances/{PREFIX}_test_predict_imp.csv', # test data: importance scores
        f'{OUT_DIR}/predict/importances/{PREFIX}_test_predict_shap_colnames.csv',
        directory(f'{OUT_DIR}/predict/importances/{PREFIX}_test_predict_shap_main.zarr')

rule run_model_refit:
    input:
        TRAIN_FILE,
        HP_DIR
    params:
        infile=TRAIN_FILE,
        threads=config['refit']['cpu_threads'],
        time=config['refit']['time'],
        mem=config['refit']['mem'],
        workers=config['refit']['workers'],
        cv_folds=config['cv']['folds'],
        out=OUT_DIR,
        train_prefix=f'{PREFIX}_train_refit',
        dask_chunks=config['jobs']['dask_row_chunks'],
        hyperparams=HP_DIR,
        gpu=config['jobs']['gpu'],
        xkey=config['jobs']['xkey'],
        ykey=config['jobs']['ykey'],
        worker_queue=config['jobs']['worker_queue'],
        job_module=config['env']['conda_module'],
        job_env=config['env']['job_env'],
        tmp_dir=f"{config['jobs']['temp_dir']}/dask_temp/{PREFIX}",
        force_n_boosters=config['refit']['force_n_boosters']
    output:
        f'{OUT_DIR}/refit/importances/{PREFIX}_train_refit_imp.csv', # importance scores
        f'{OUT_DIR}/refit/importances/{PREFIX}_train_refit_shap_colnames.csv',
        directory(f'{OUT_DIR}/refit/importances/{PREFIX}_train_refit_shap_main.zarr'),
        f'{OUT_DIR}/refit/models/{PREFIX}_train_refit_origrefit_xgbmodel.json', # models
        f'{OUT_DIR}/refit/models/{PREFIX}_train_refit_shaprefit_xgbmodel.json',
        f'{OUT_DIR}/refit/predictors/{PREFIX}_train_refit_used_cols.csv', # predictors
        temp(directory(f"{config['jobs']['temp_dir']}/dask_temp/{PREFIX}")) # dask temp dir - to be deleted
    shell:
        """
        # load module and env
        module purge
        module load {params.job_module}
        conda activate {params.job_env}

        # print node name for setting up dask dashboard
        echo -e "\n################################################################################"
        echo -e "############################# DASK DASHBOARD SETUP #############################"
        echo "  Running Dask scheduler on node ${{SLURMD_NODENAME}}"
        echo "  Paste command below into local terminal session to allow tunnelling for the dask dashboard:"
        echo "  'ssh -N -L 8787:${{SLURMD_NODENAME}}:8787 username@server'"
        echo "  paste address 'localhost:8787' in local browser session"
        echo -e "################################################################################"
        echo -e "################################################################################\n"

        python3 ../../scripts/refit.py \
            --in_ml {params.infile} \
            --n_threads_per_worker {params.threads} \
            --time_per_worker {params.time} \
            --mem_per_worker {params.mem} \
            --n_workers_in_cluster {params.workers} \
            --n_folds {params.cv_folds} \
            --out {params.out} \
            --prefix {params.train_prefix} \
            --local_dir {params.tmp_dir} \
            --row_chunk_size {params.dask_chunks} \
            --cluster distributed \
            --incremental_learning False \
            --incremental_start_round 0 \
            --incremental_n_boost_per_round 10 \
            --verbose True \
            --xgb_eval_metric logloss \
            --run_shap_main True \
            --run_shap_inter False \
            --hp_search_results {params.hyperparams} \
            --gpu {params.gpu} \
            --interface None \
            --xkey {params.xkey} \
            --ykey {params.ykey} \
            --worker_queue {params.worker_queue} \
            --n_booster_overide {params.force_n_boosters}
        """

rule run_model_predict:
    input:
        TEST_FILE,
        bst_col_path=f'{OUT_DIR}/refit/predictors/{PREFIX}_train_refit_used_cols.csv',
        bst_path=f'{OUT_DIR}/refit/models/{PREFIX}_train_refit_shaprefit_xgbmodel.json'
    params:
        infile=TEST_FILE,
        bst_col_path=f'{OUT_DIR}/refit/predictors/{PREFIX}_train_refit_used_cols.csv',
        bst_path=f'{OUT_DIR}/refit/models/{PREFIX}_train_refit_shaprefit_xgbmodel.json',
        platt_model=PLATT_MODEL,
        threads=config['predict']['cpu_threads'],
        time=config['predict']['time'],
        mem=config['predict']['mem'],
        workers=config['predict']['workers'],
        gpu=config['jobs']['gpu'],
        xkey=config['jobs']['xkey'],
        ykey=config['jobs']['ykey'],
        worker_queue=config['jobs']['worker_queue'],
        dask_chunks=config['jobs']['dask_row_chunks'],
        out=OUT_DIR,
        test_prefix=f'{PREFIX}_test_predict',
        job_module=config['env']['conda_module'],
        job_env=config['env']['job_env'],
        tmp_dir=f"{config['jobs']['temp_dir']}/dask_temp/{PREFIX}"
    output:
        temp(directory(f"{config['jobs']['temp_dir']}/dask_temp/{PREFIX}")), # temp dask files
        f'{OUT_DIR}/predict/predictions/{PREFIX}_test_predict_y_pred.csv', # predictions
        f'{OUT_DIR}/predict/importances/{PREFIX}_test_predict_imp.csv', # importance scores
        f'{OUT_DIR}/predict/importances/{PREFIX}_test_predict_shap_colnames.csv',
        directory(f'{OUT_DIR}/predict/importances/{PREFIX}_test_predict_shap_main.zarr')
    shell:
        """
        # load module and env
        module purge
        module load {params.job_module}
        conda activate {params.job_env}

        # print node name for setting up dask dashboard
        echo -e "\n################################################################################"
        echo -e "############################# DASK DASHBOARD SETUP #############################"
        echo "  Running Dask scheduler on node ${{SLURMD_NODENAME}}"
        echo "  Paste command below into local terminal session to allow tunnelling for the dask dashboard:"
        echo "  'ssh -N -L 8787:${{SLURMD_NODENAME}}:8787 username@server'"
        echo "  paste address 'localhost:8787' in local browser session"
        echo -e "################################################################################"
        echo -e "################################################################################\n"

        python3 ../../scripts/predict.py \
            --in_ml {params.infile} \
            --used_cols {params.bst_col_path} \
            --bst_path {params.bst_path} \
            --cluster distributed \
            --n_threads_per_worker {params.threads} \
            --time_per_worker {params.time} \
            --mem_per_worker {params.mem} \
            --n_workers_in_cluster {params.workers} \
            --out {params.out} \
            --prefix {params.test_prefix} \
            --local_dir {params.tmp_dir} \
            --row_chunk_size {params.dask_chunks} \
            --run_shap_main True \
            --run_shap_inter False \
            --platt_scale_model {params.platt_model} \
            --gpu {params.gpu} \
            --interface None \
            --xkey {params.xkey} \
            --ykey {params.ykey} \
            --worker_queue {params.worker_queue}
        """