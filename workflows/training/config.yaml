input:
  data:
    train: "/home/matthew.smith/daxos/examples/data/example_data_train.hdf5"
    test: "/home/matthew.smith/daxos/examples/data/example_data_test.hdf5"
# set default arguments for when args are not passed

output: # ??? this is a mess - sort to single output folder
  dir: "/home/matthew.smith/workdir2024/xgboost"
  prefix: "test_run_adjusted" # run name to append to output folder

cv:
  folds: 3 # n folds in CV
  iter: 50 # n iterations random search in CV
  time: "'04_00_00'" # time per worker as HH_MM_SS
  mem: "'32G'" # mem per worker as G
  workers: 4 # n workers
  cvsubsample: 5000 # size of train data to take as subsample for HP search
  hyperparams: None # ???
  hp_round: 0 # ??? which round of HP tuning we're on - may be redundant with snakemake

refit: # resources must cover total of 640G RAM for 28k*215k train set
  cpu_threads: 2
  time: "'01_00_00'" 
  mem: "'20G'"
  workers: 1
  force_n_boosters: 10
  hyperparams: "/home/matthew.smith/workdir2024/xgboost/cv/clumped_075_maf_005_with_apoe/hpsearch" # path to file or directors with output from HP search

predict:
  cpu_threads: 2
  time: "'00_30_00'" 
  mem: "'10G'"
  workers: 1
  bst_path: None # "/path/to/booster.json" path to prefit serialised boosting model. Default to output from refit if None. Put here to overwrite
  bst_col_path: None # "/path/to/booster/columns.txt" file containing column names from model in bst. Default to output from refit if None. Put here to overwrite
  platt_model: None # "/path/to/model.json" for the platt scale model. Default None uses model from refit. Put here to overwrite.

jobs: # holds universal rules for runs
  dask_row_chunks: 500 # size of chunks to read with dask
  interface: None # can manually try to force ethernet, infiniband etc.
  worker_queue: None # can manually try to force worker queue
  xkey: 'x_adjusted' # key for accessing predictors in hdf5 files
  ykey: 'y_adjusted' # key for accessing outcome in hdf5 files
  seed: 123
  gpu: True # If changed, then profiles/slurm/config.yaml must also change to account for this
  gpu_resources: 'gpu:1' # specify as <resource>:<type>:<count>, where type is optional. Passed to --gres=
  temp_dir: '/home/matthew.smith/workdir2024' # must also be changed in slurm_extra in profiles/slurm/config.yaml

model: # holds universal rules for xgb models
  subsample: 0.7 # size of subsample for each tree in boosting rounds
  boosters: 1000 # number of boosting rounds
  run_shap_inter: False # if True then export SHAP interaction values - v. high mem req. and some concurrency issues seen
  xgb_model_type: "shap" # ??? presume this is how to extract importance scores?

env:
  conda_module: "conda/23.11-py311" # your base conda module to load on an HPC environment
  job_env: "daxos" # your environement to load for runs. Should be base daxos env from daxos/requirements.yaml. 