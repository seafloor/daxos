input:
  data:
    train: "../../examples/data/example_data.hdf5"
    test: "/your/test/data.hdf5"
# set default arguments for when args are not passed

output: # ??? this is a mess - sort to single output folder
  dir: "/home/matthew.smith/workdir2024/xgboost"
  prefix: "test_run" # run name to append to output folder

cv:
  folds: 5 # n folds in CV
  iter: 50 # n iterations random search in CV
  time: "'04_00_00'" # time per worker as HH_MM_SS
  mem: "'32G'" # mem per worker as G
  workers: 4 # n workers
  cvsubsample: 5000 # size of train data to take as subsample for HP search
  hyperparams: None # ???
  hp_round: 0 # ??? which round of HP tuning we're on - may be redundant with snakemake

refit: # resources must cover total of 640G RAM for 28k*215k train set
  cpu_threads: 5
  time: "'20_00_00'" 
  mem: "'50G'"
  workers: 13
  hyperparams: "/home/matthew.smith/workdir2024/xgboost/cv/clumped_075_maf_005_with_apoe/hpsearch" # path to file or directors with output from HP search

predict:
  bst: "/path/to/booster.json" # path to prefit serialised boosting model to use for predictions
  bstcols: "path/to/booster/columns.txt" # file containing column names from model in bst

jobs: # holds universal rules for runs
  dask_row_chunks: 500 # size of chunks to read with dask
  interface: None # can manually try to force ethernet, infiniband etc.
  worker_queue: None # can manually try to force worker queue
  xkey: 'x' # key for accessing predictors in hdf5 files
  ykey: 'y' # key for accessing outcome in hdf5 files
  seed: 123
  gpu: False # if true run all training on GPU. NOT IMPLEMENTED.

model: # holds universal rules for xgb models
  subsample: 0.7 # size of subsample for each tree in boosting rounds
  boosters: 1000 # number of boosting rounds
  run_shap_inter: False # if True then export SHAP interaction values - v. high mem req. and some concurrency issues seen
  xgb_model_type: "shap" # ??? presume this is how to extract importance scores?

env:
  conda_module: "conda/23.11-py311" # your base conda module to load on an HPC environment
  job_env: "daxos" # your environement to load for runs. Should be base daxos env from daxos/requirements.yaml. 